# -----------------------------
# 1. Káº¿t ná»‘i API
# -----------------------------
import pandas as pd
import streamlit as st
import faiss
import numpy as np
import pickle
import json
import os
import unicodedata
import re

from openai import OpenAI
os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
os.environ["OPENAI_API_KEY"] = "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"

client = OpenAI()
#client = OpenAI(
   # base_url="https://openrouter.ai/api/v1",
   # api_key="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"
#)


# -----------------------------
# CACHE CSV
# -----------------------------
@st.cache_data
def load_csv():
    df = pd.read_csv("data.csv")
    df["Loáº¡i hÃ¬nh list"] = df["Loáº¡i hÃ¬nh"].apply(
        lambda x: [i.strip() for i in x.split("/")]
    )
    df["text"] = df.apply(
        lambda x: f"{x['TÃªn Ä‘á»‹a Ä‘iá»ƒm']} | {', '.join(x['Loáº¡i hÃ¬nh list'])}",
        axis=1
    )
    return df

df = load_csv()


# -----------------------------
# CACHE FAISS & PICKLE
# -----------------------------
@st.cache_resource
def load_faiss_and_chunks():
    fact_index = faiss.read_index("vector_index.faiss")
    with open("chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
    with open("metadata.pkl", "rb") as f:
        metadata = pickle.load(f)
    return fact_index, chunks, metadata

fact_index, chunks, metadata = load_faiss_and_chunks()


# -----------------------------
# 3. Preference functions
# -----------------------------
PREF_FILE = "user_preferences.json"

def load_prefs():
    if not os.path.exists(PREF_FILE):
        return {}
    return json.load(open(PREF_FILE, "r", encoding="utf-8"))

def save_prefs(p):
    json.dump(p, open(PREF_FILE, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

# -----------------------------
# UI â€“ chá»n sá»Ÿ thÃ­ch
# -----------------------------
st.title("LumiAI â€“ Thiáº¿t láº­p sá»Ÿ thÃ­ch du lá»‹ch")

travel_types = ["TÃ´n giÃ¡o","Sinh thÃ¡i","Dá»‹ch vá»¥","Miá»‡t vÆ°á»n","Kiáº¿n trÃºc","VÄƒn hoÃ¡","TÃ¢m linh","Di tÃ­ch"]

interests = st.multiselect("Chá»n loáº¡i hÃ¬nh du lá»‹ch:", travel_types)

# -----------------------------
# LÆ°u sá»Ÿ thÃ­ch + Gá»£i Ã½ ngay
# -----------------------------

def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    # chuáº©n hÃ³a unicode, tÃ¡ch dáº¥u, bá» dáº¥u (Ä‘á»ƒ so sÃ¡nh khÃ´ng phÃ¢n biá»‡t dáº¥u)
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")  # remove accents
    s = s.lower()
    s = re.sub(r"[^\w\s]", " ", s)   # thay kÃ½ tá»± Ä‘áº·c biá»‡t báº±ng space
    s = re.sub(r"\s+", " ", s).strip()
    return s


@st.cache_data
def normalize_categories(df):
    def split_and_normalize(cat):
        parts = [p.strip() for p in str(cat).split("/")]
        return [normalize_text(p) for p in parts]
    df["Loáº¡i hÃ¬nh list norm"] = df["Loáº¡i hÃ¬nh"].apply(split_and_normalize)
    return df

df = normalize_categories(df)


prefs = load_prefs()
pref_norms = [i.lower() for i in prefs["interests"]]

def is_match(row_cats, pref_norms):
    return len(set(row_cats) & set(pref_norms)) > 0

df["match"] = df["Loáº¡i hÃ¬nh list norm"].apply(lambda cats: is_match(cats, pref_norms))

df_matched = df[df["match"] == True]
df_sorted = df_matched  # náº¿u khÃ´ng muá»‘n xáº¿p háº¡ng thÃ¬ bá» Sorting


# --- UI handler ---
if st.button("LÆ°u sá»Ÿ thÃ­ch"):
    user_pref = {
        "interests": interests  # lÆ°u nguyÃªn gá»‘c Ä‘á»ƒ hiá»ƒn thá»‹
    }
    # LÆ°u cáº£ dáº¡ng chuáº©n hÃ³a Ä‘á»ƒ so sÃ¡nh nhanh
    user_pref["_interests_norm"] = [normalize_text(i) for i in interests]
    save_prefs(user_pref)
    st.success("ÄÃ£ lÆ°u thÃ´ng tin sá»Ÿ thÃ­ch!")

    # Chuáº©n hÃ³a cá»™t Loáº¡i hÃ¬nh list 1 láº§n (an toÃ n náº¿u cháº¡y nhiá»u láº§n)
    def split_and_normalize(cat):
        parts = [p.strip() for p in str(cat).split("/")]
        return [normalize_text(p) for p in parts if p]

    df["Loáº¡i hÃ¬nh list norm"] = df["Loáº¡i hÃ¬nh"].apply(split_and_normalize)

    prefs = load_prefs()
    pref_norms = prefs.get("_interests_norm", [])

    def match_score(row_norms, pref_norms):
        # dÃ¹ng giao (intersection) trÃªn táº­p Ä‘á»ƒ cho Ä‘iá»ƒm cÃ´ng báº±ng
        set_row = set(row_norms)
        set_pref = set(pref_norms)
        return len(set_row & set_pref)

    # tÃ­nh score dá»±a trÃªn cá»™t chuáº©n hÃ³a
    df["score"] = df["Loáº¡i hÃ¬nh list norm"].apply(lambda r: match_score(r, pref_norms))
    df_sorted = df[df["score"] > 0].sort_values(by="score", ascending=False)

    # náº¿u muá»‘n show top 10, nhÆ°ng Ä‘áº£m báº£o chá»‰ láº¥y nhá»¯ng hÃ ng cÃ³ score>0
    top_df = df_sorted.head(10)
    if top_df.empty:
        st.info("KhÃ´ng tÃ¬m tháº¥y Ä‘á»‹a Ä‘iá»ƒm phÃ¹ há»£p vá»›i sá»Ÿ thÃ­ch cá»§a báº¡n trong dataset.")
    else:
        location_summaries = "\n\n".join(top_df["text"].tolist())

        # gá»i model (giá»¯ nguyÃªn pháº§n gá»i API cá»§a anh náº¿u muá»‘n)
        # CHÃš Ã Báº¢O Máº¬T: KHÃ”NG Ä‘áº·t API KEY cá»©ng trong mÃ£ nguá»“n; dÃ¹ng biáº¿n mÃ´i trÆ°á»ng hoáº·c secret manager.
        system_prompt = f"""
You are LumiAI â€” an AI for tourism, culture, and fact verification.
Your tasks:
User travel preferences:
    {user_pref}
-Using the CSV dataset:
    +Identify related locations
    +Provide travel recommendations
    +Describe features, activities, and experiences
"""
        user_message = (
            "Dá»¯ liá»‡u (chá»‰ sá»­ dá»¥ng nhá»¯ng dÃ²ng bÃªn dÆ°á»›i Ä‘á»ƒ tráº£ lá»i):\n\n"
            f"{location_summaries}\n\n"
            "YÃªu cáº§u: Dá»±a trÃªn dá»¯ liá»‡u á»Ÿ trÃªn, hÃ£y liá»‡t kÃª gá»£i Ã½ du lá»‹ch sÃ¡t vá»›i sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng (khÃ´ng thÃªm thá»«a), mÃ´ táº£ ngáº¯n má»—i nÆ¡i (2-3 cÃ¢u)."
        )

        completion = client.chat.completions.create(
            model="meta-llama/llama-3.3-70b-instruct:free",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
        )
        suggestion = completion.choices[0].message.content
        st.session_state.suggestion = suggestion
        # LÆ°u lá»‹ch sá»­ gá»£i Ã½
        if "suggestions_history" not in st.session_state:
            st.session_state.suggestions_history = []

        st.session_state.suggestions_history.append(suggestion)



if "suggestion" in st.session_state:
    st.markdown("### âœ¨ Gá»£i Ã½ du lá»‹ch tá»« LumiAI")
    st.write(st.session_state.suggestion)

suggestion = st.session_state.get("suggestion", "")
# -----------------------------
# Chat liÃªn tá»¥c
# -----------------------------
st.write("---")
st.subheader("ğŸ’¬ Há»i LumiAI thÃªm:")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Nháº­p yÃªu cáº§u cá»§a báº¡n...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    prefs = load_prefs()

    # Láº¥y embedding
    query_emb = client.embeddings.create(
        model="text-embedding-3-small",
        input=user_input
    ).data[0].embedding

    q = np.array([query_emb]).astype("float32")


    #D1, I1 = suggestion.search(q, k=10)
    #valid_indices = [i for i in I1[0] if 0 <= i < len(df)]
    #retrieved_csv = "\n".join(df.iloc[i]["text"] for i in valid_indices)

    # TÃ¬m trong CHUNKS
    D2, I2 = fact_index.search(q, k=10)
    retrieved_chunks = ""
    for i in I2[0]:
        if 0 <= i < len(chunks):
            m = metadata[i]
            retrieved_chunks += f"\n\n[Chunk {i} â€” Lines {m['start_line']}â€“{m['end_line']}]:\n{chunks[i]}"

    # System prompt chat
    system_prompt = f"""
You are LumiAI â€” an AI for tourism, culture, and fact verification.
Your tasks:
-Using the CSV dataset:
    +Identify related locations
    +Provide travel recommendations
    +Describe features, activities, and experiences
-Using the CHUNKS dataset:
    + Deliver cultural, historical, and mythological explanations
    + Include chunk, lines, page (the number which stands alone after the information) after the response
    + Always include the exact source for every statement.
    + If multiple statements use multiple chunks,pages, list all sources
    + Never invent sources
    + Verify correctness if the question requires fact-checking
    + Absolutely no fabrication of facts.
    + If the data is missing â†’ respond with â€œKhÃ´ng cÃ³ dá»¯ liá»‡uâ€.
    + Add link, source for the tour

You operate in CLOSED-BOOK MODE.
You are ONLY allowed to speak using information that appears in the datasets below.
You are forbidden from using world knowledge, memory, training data, or assumptions.
If the answer requires any information that is not explicitly present in the dataset,
you must respond with EXACTLY: â€œKhÃ´ng cÃ³ dá»¯ liá»‡uâ€.
You must quote the exact chunk + line numbers for every fact used.

Suggestion message, you need to use if user ask according to your previous suggestion:
{suggestion}

Cultural Data:
{retrieved_chunks}
"""

    completion = client.chat.completions.create(
        model="meta-llama/llama-3.3-70b-instruct:free",
        messages=[
            {"role": "system", "content": system_prompt},
            *st.session_state.messages[-5:],  # giá»¯ lá»‹ch sá»­ ngáº¯n
            {"role": "user", "content": user_input}
        ]
    )

    ai_msg = completion.choices[0].message.content
    st.session_state.messages.append({"role": "assistant", "content": ai_msg})

    with st.chat_message("assistant"):
        st.markdown(ai_msg)