# -----------------------------
# 1. K·∫øt n·ªëi API
# -----------------------------
import pandas as pd
import streamlit as st
import faiss
import numpy as np
import pickle
import json
import os
import unicodedata
import re

from openai import OpenAI
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=st.secrets["api"]
)
# -----------------------------
# CACHE CSV
# -----------------------------
@st.cache_data
def load_csv():
    df = pd.read_csv("data.csv")
    df["Lo·∫°i h√¨nh list"] = df["Lo·∫°i h√¨nh"].apply(
        lambda x: [i.strip() for i in x.split("/")]
    )
    df["text"] = df.apply(
        lambda x: f"{x['T√™n ƒë·ªãa ƒëi·ªÉm']} | {', '.join(x['Lo·∫°i h√¨nh list'])}",
        axis=1
    )
    return df

df = load_csv()


# -----------------------------
# CACHE FAISS & PICKLE
# -----------------------------
@st.cache_resource
def load_faiss_and_chunks():
    fact_index = faiss.read_index("vector_index.faiss")
    with open("chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
    with open("metadata.pkl", "rb") as f:
        metadata = pickle.load(f)
    return fact_index, chunks, metadata

fact_index, chunks, metadata = load_faiss_and_chunks()


# -----------------------------
# 3. Preference functions
# -----------------------------
PREF_FILE = "user_preferences.json"

def load_prefs():
    if not os.path.exists(PREF_FILE):
        return {}
    return json.load(open(PREF_FILE, "r", encoding="utf-8"))

def save_prefs(p):
    json.dump(p, open(PREF_FILE, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

# -----------------------------
# UI ‚Äì ch·ªçn s·ªü th√≠ch
# -----------------------------
st.title("LumiAI ‚Äì Thi·∫øt l·∫≠p s·ªü th√≠ch du l·ªãch")

travel_types = ["T√¥n gi√°o","Sinh th√°i","D·ªãch v·ª•","Mi·ªát v∆∞·ªùn","Ki·∫øn tr√∫c","VƒÉn ho√°","T√¢m linh","Di t√≠ch"]

interests = st.multiselect("Ch·ªçn lo·∫°i h√¨nh du l·ªãch:", travel_types)

# -----------------------------
# L∆∞u s·ªü th√≠ch + G·ª£i √Ω ngay
# -----------------------------

def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    # chu·∫©n h√≥a unicode, t√°ch d·∫•u, b·ªè d·∫•u (ƒë·ªÉ so s√°nh kh√¥ng ph√¢n bi·ªát d·∫•u)
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")  # remove accents
    s = s.lower()
    s = re.sub(r"[^\w\s]", " ", s)   # thay k√Ω t·ª± ƒë·∫∑c bi·ªát b·∫±ng space
    s = re.sub(r"\s+", " ", s).strip()
    return s


@st.cache_data
def normalize_categories(df):
    def split_and_normalize(cat):
        parts = [p.strip() for p in str(cat).split("/")]
        return [normalize_text(p) for p in parts]
    df["Lo·∫°i h√¨nh list norm"] = df["Lo·∫°i h√¨nh"].apply(split_and_normalize)
    return df

df = normalize_categories(df)


prefs = load_prefs()
pref_norms = [i.lower() for i in prefs.get("interests", [])]

def is_match(row_cats, pref_norms):
    return len(set(row_cats) & set(pref_norms)) > 0

df["match"] = df["Lo·∫°i h√¨nh list norm"].apply(lambda cats: is_match(cats, pref_norms))

df_matched = df[df["match"] == True]
df_sorted = df_matched  # n·∫øu kh√¥ng mu·ªën x·∫øp h·∫°ng th√¨ b·ªè Sorting


# --- UI handler ---
if st.button("L∆∞u s·ªü th√≠ch"):
    user_pref = {
        "interests": interests  # l∆∞u nguy√™n g·ªëc ƒë·ªÉ hi·ªÉn th·ªã
    }
    # L∆∞u c·∫£ d·∫°ng chu·∫©n h√≥a ƒë·ªÉ so s√°nh nhanh
    user_pref["_interests_norm"] = [normalize_text(i) for i in interests]
    save_prefs(user_pref)
    st.success("ƒê√£ l∆∞u th√¥ng tin s·ªü th√≠ch!")

    # Chu·∫©n h√≥a c·ªôt Lo·∫°i h√¨nh list 1 l·∫ßn (an to√†n n·∫øu ch·∫°y nhi·ªÅu l·∫ßn)
    def split_and_normalize(cat):
        parts = [p.strip() for p in str(cat).split("/")]
        return [normalize_text(p) for p in parts if p]

    df["Lo·∫°i h√¨nh list norm"] = df["Lo·∫°i h√¨nh"].apply(split_and_normalize)

    prefs = load_prefs()
    pref_norms = prefs.get("_interests_norm", [])

    def match_score(row_norms, pref_norms):
        # d√πng giao (intersection) tr√™n t·∫≠p ƒë·ªÉ cho ƒëi·ªÉm c√¥ng b·∫±ng
        set_row = set(row_norms)
        set_pref = set(pref_norms)
        return len(set_row & set_pref)

    # t√≠nh score d·ª±a tr√™n c·ªôt chu·∫©n h√≥a
    df["score"] = df["Lo·∫°i h√¨nh list norm"].apply(lambda r: match_score(r, pref_norms))
    df_sorted = df[df["score"] > 0].sort_values(by="score", ascending=False)

    # n·∫øu mu·ªën show top 10, nh∆∞ng ƒë·∫£m b·∫£o ch·ªâ l·∫•y nh·ªØng h√†ng c√≥ score>0
    top_df = df_sorted.head(10)
    if top_df.empty:
        st.info("Kh√¥ng t√¨m th·∫•y ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p v·ªõi s·ªü th√≠ch c·ªßa b·∫°n trong dataset.")
    else:
        location_summaries = "\n\n".join(top_df["text"].tolist())

        # g·ªçi model (gi·ªØ nguy√™n ph·∫ßn g·ªçi API c·ªßa anh n·∫øu mu·ªën)
        # CH√ö √ù B·∫¢O M·∫¨T: KH√îNG ƒë·∫∑t API KEY c·ª©ng trong m√£ ngu·ªìn; d√πng bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c secret manager.
        system_prompt = f"""
You are LumiAI ‚Äî an AI for tourism, culture, and fact verification.
Your tasks:
User travel preferences:
    {user_pref}
-Using the CSV dataset:
    +Identify related locations
    +Provide travel recommendations
    +Describe features, activities, and experiences
ONLY RESPOND IN VIETNAMESE
"""
        user_message = (
            "Data (only use this bellow for your responses):\n\n"
            f"{location_summaries}\n\n"
            "Task: Based on the data above, list travel suggestions that closely match the user‚Äôs preferences (without adding anything extra)."
        )

        completion = client.chat.completions.create(
            #model="meta-llama/llama-3.3-70b-instruct:free",
            model="nvidia/nemotron-nano-12b-v2-vl:free",
            #model="qwen/qwen2.5-7b-instruct:free",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
        )
        suggestion = completion.choices[0].message.content
        st.session_state.suggestion = suggestion
        # L∆∞u l·ªãch s·ª≠ g·ª£i √Ω
        if "suggestions_history" not in st.session_state:
            st.session_state.suggestions_history = []

        st.session_state.suggestions_history.append(suggestion)



if "suggestion" in st.session_state:
    st.markdown("### ‚ú® G·ª£i √Ω du l·ªãch t·ª´ LumiAI")
    st.write(st.session_state.suggestion)

suggestion = st.session_state.get("suggestion", "")
# -----------------------------
# Chat li√™n t·ª•c
# -----------------------------
st.write("---")
st.subheader("üí¨ H·ªèi LumiAI th√™m:")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Nh·∫≠p y√™u c·∫ßu c·ªßa b·∫°n...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    prefs = load_prefs()

    # L·∫•y embedding
    query_emb = client.embeddings.create(
        model="text-embedding-3-small",
        input=user_input
    ).data[0].embedding

    q = np.array([query_emb]).astype("float32")
    # T√¨m trong CHUNKS
    D2, I2 = fact_index.search(q, k=10)
    retrieved_chunks = ""
    for i in I2[0]:
        if 0 <= i < len(chunks):
            m = metadata[i]
            retrieved_chunks += f"\n\n[Chunk {i} ‚Äî Lines {m['start_line']}‚Äì{m['end_line']}]:\n{chunks[i]}"

    # System prompt chat
    system_prompt = f"""
You are LumiAI ‚Äî an AI for tourism, culture, and fact verification.
Your tasks:
-Using the CSV dataset:
    +Identify related locations
    +Provide travel recommendations
    +Describe features, activities, and experiences
-Using the CHUNKS dataset:
    + Deliver cultural, historical, and mythological explanations
    + Include chunk, lines, page (the number which stands alone after the information) after the response
    + Always include the exact source for every statement.
    + If multiple statements use multiple chunks,pages, list all sources
    + Never invent sources
    + Verify correctness if the question requires fact-checking
    + Absolutely no fabrication of facts.
    + If the data is missing ‚Üí respond with ‚ÄúKh√¥ng c√≥ d·ªØ li·ªáu‚Äù.
    + Add link, source for the tour

You operate in CLOSED-BOOK MODE.
You are ONLY allowed to speak using information that appears in the datasets below.
You are forbidden from using world knowledge, memory, training data, or assumptions.
If the answer requires any information that is not explicitly present in the dataset,
you must respond with EXACTLY: ‚ÄúKh√¥ng c√≥ d·ªØ li·ªáu‚Äù.
You must quote the exact chunk + line numbers for every fact used.
ONLY RESPOND IN VIETNAMESE
Suggestion message, you need to use if user ask according to your previous suggestion:
{suggestion}

Cultural Data:
{retrieved_chunks}
AGAIN DO NOT MAKE UP INFORMATION
"""

    completion = client.chat.completions.create(
        #model="meta-llama/llama-3.3-70b-instruct:free",
        model="nvidia/nemotron-nano-12b-v2-vl:free",
        #model="qwen/qwen2.5-7b-instruct:free",
        messages=[
            {"role": "system", "content": system_prompt},
            *st.session_state.messages[-5:],  # gi·ªØ l·ªãch s·ª≠ ng·∫Øn
            {"role": "user", "content": user_input}
        ]
    )

    ai_msg = completion.choices[0].message.content
    st.session_state.messages.append({"role": "assistant", "content": ai_msg})

    with st.chat_message("assistant"):
        st.markdown(ai_msg)